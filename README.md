# Lipschitz Transformer PyTorch

## The Lipschitz Constant of Self-Attention
Unofficial implementation of L2 Attention module.       


## LRFormer: Mitigating Transformer Overconfidence via Lipschitz Regularization
Official implementation of LRSA module in LRFormer. (We are the author teams)       
This is the LRSA module, please replace it to any DP Attention module and conduct experiments.     
